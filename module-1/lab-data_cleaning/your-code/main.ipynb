{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import pymysql and sqlalchemy as you have learnt in the lesson of importing/exporting data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# had to update my Jupyter notebook to be using Python 3\n",
    "# python3 -m pip install ipykernel\n",
    "# python3 -m ipykernel install --user\n",
    "# ModuleNotFoundError: No module named 'pymysql'\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a mysql engine to set the connection to the server. Check the connection details in [this link](https://relational.fit.cvut.cz/search?tableCount%5B%5D=0-10&tableCount%5B%5D=10-30&dataType%5B%5D=Numeric&databaseSize%5B%5D=KB&databaseSize%5B%5D=MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('mysql+pymysql://guest:relational@relational.fit.cvut.cz/stats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Import the users table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Reputation        CreationDate   DisplayName      LastAccessDate  \\\n",
      "0  -1           1 2010-07-19 06:55:26     Community 2010-07-19 06:55:26   \n",
      "1   2         101 2010-07-19 14:01:36  Geoff Dalgas 2013-11-12 22:07:23   \n",
      "2   3         101 2010-07-19 15:34:50  Jarrod Dixon 2014-08-08 06:42:58   \n",
      "3   4         101 2010-07-19 19:03:27        Emmett 2014-01-02 09:31:02   \n",
      "4   5        6792 2010-07-19 19:03:57         Shane 2014-08-13 00:23:47   \n",
      "\n",
      "                       WebsiteUrl            Location  \\\n",
      "0  http://meta.stackexchange.com/  on the server farm   \n",
      "1        http://stackoverflow.com       Corvallis, OR   \n",
      "2        http://stackoverflow.com        New York, NY   \n",
      "3    http://minesweeperonline.com   San Francisco, CA   \n",
      "4         http://www.statalgo.com        New York, NY   \n",
      "\n",
      "                                             AboutMe  Views  UpVotes  \\\n",
      "0  <p>Hi, I'm not really a person.</p>\\n\\n<p>I'm ...      0     5007   \n",
      "1  <p>Developer on the StackOverflow team.  Find ...     25        3   \n",
      "2  <p><a href=\"http://blog.stackoverflow.com/2009...     22       19   \n",
      "3  <p>currently at a startup in SF</p>\\n\\n<p>form...     11        0   \n",
      "4  <p>Quantitative researcher focusing on statist...   1145      662   \n",
      "\n",
      "   DownVotes  AccountId   Age                     ProfileImageUrl  \n",
      "0       1920         -1   NaN                                None  \n",
      "1          0          2  37.0                                None  \n",
      "2          0          3  35.0                                None  \n",
      "3          0       1998  28.0  http://i.stack.imgur.com/d1oHX.jpg  \n",
      "4          5      54503  35.0                                None  \n"
     ]
    }
   ],
   "source": [
    "raw_users = pd.read_sql_query('SELECT * FROM stats.users', engine)\n",
    "# print(raw_users.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Rename Id column to userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  Reputation        CreationDate   DisplayName      LastAccessDate  \\\n",
      "0      -1           1 2010-07-19 06:55:26     Community 2010-07-19 06:55:26   \n",
      "1       2         101 2010-07-19 14:01:36  Geoff Dalgas 2013-11-12 22:07:23   \n",
      "2       3         101 2010-07-19 15:34:50  Jarrod Dixon 2014-08-08 06:42:58   \n",
      "3       4         101 2010-07-19 19:03:27        Emmett 2014-01-02 09:31:02   \n",
      "4       5        6792 2010-07-19 19:03:57         Shane 2014-08-13 00:23:47   \n",
      "\n",
      "                       WebsiteUrl            Location  \\\n",
      "0  http://meta.stackexchange.com/  on the server farm   \n",
      "1        http://stackoverflow.com       Corvallis, OR   \n",
      "2        http://stackoverflow.com        New York, NY   \n",
      "3    http://minesweeperonline.com   San Francisco, CA   \n",
      "4         http://www.statalgo.com        New York, NY   \n",
      "\n",
      "                                             AboutMe  Views  UpVotes  \\\n",
      "0  <p>Hi, I'm not really a person.</p>\\n\\n<p>I'm ...      0     5007   \n",
      "1  <p>Developer on the StackOverflow team.  Find ...     25        3   \n",
      "2  <p><a href=\"http://blog.stackoverflow.com/2009...     22       19   \n",
      "3  <p>currently at a startup in SF</p>\\n\\n<p>form...     11        0   \n",
      "4  <p>Quantitative researcher focusing on statist...   1145      662   \n",
      "\n",
      "   DownVotes  AccountId   Age                     ProfileImageUrl  \n",
      "0       1920         -1   NaN                                None  \n",
      "1          0          2  37.0                                None  \n",
      "2          0          3  35.0                                None  \n",
      "3          0       1998  28.0  http://i.stack.imgur.com/d1oHX.jpg  \n",
      "4          5      54503  35.0                                None  \n"
     ]
    }
   ],
   "source": [
    "users_with_id = raw_users.rename(columns = {'Id': 'userId'})\n",
    "# print(raw_users.head())\n",
    "print(users_with_id.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Import the posts table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  PostTypeId  AcceptedAnswerId         CreaionDate  Score  ViewCount  \\\n",
      "0   1           1              15.0 2010-07-19 19:12:12     23     1278.0   \n",
      "1   2           1              59.0 2010-07-19 19:12:57     22     8198.0   \n",
      "2   3           1               5.0 2010-07-19 19:13:28     54     3613.0   \n",
      "3   4           1             135.0 2010-07-19 19:13:31     13     5224.0   \n",
      "4   5           2               NaN 2010-07-19 19:14:43     81        NaN   \n",
      "\n",
      "                                                Body  OwnerUserId  \\\n",
      "0  <p>How should I elicit prior distributions fro...          8.0   \n",
      "1  <p>In many different statistical methods there...         24.0   \n",
      "2  <p>What are some valuable Statistical Analysis...         18.0   \n",
      "3  <p>I have two groups of data.  Each with a dif...         23.0   \n",
      "4  <p>The R-project</p>\\n\\n<p><a href=\"http://www...         23.0   \n",
      "\n",
      "      LasActivityDate                                              Title  ...  \\\n",
      "0 2010-09-15 21:08:26                      Eliciting priors from experts  ...   \n",
      "1 2012-11-12 09:21:54                                 What is normality?  ...   \n",
      "2 2013-05-27 14:48:36  What are some valuable Statistical Analysis op...  ...   \n",
      "3 2010-09-08 03:00:19  Assessing the significance of differences in d...  ...   \n",
      "4 2010-07-19 19:21:15                                               None  ...   \n",
      "\n",
      "  AnswerCount  CommentCount  FavoriteCount  LastEditorUserId  \\\n",
      "0         5.0             1           14.0               NaN   \n",
      "1         7.0             1            8.0              88.0   \n",
      "2        19.0             4           36.0             183.0   \n",
      "3         5.0             2            2.0               NaN   \n",
      "4         NaN             3            NaN              23.0   \n",
      "\n",
      "         LastEditDate  CommunityOwnedDate ParentId  ClosedDate  \\\n",
      "0                 NaT                 NaT      NaN         NaT   \n",
      "1 2010-08-07 17:56:44                 NaT      NaN         NaT   \n",
      "2 2011-02-12 05:50:03 2010-07-19 19:13:28      NaN         NaT   \n",
      "3                 NaT                 NaT      NaN         NaT   \n",
      "4 2010-07-19 19:21:15 2010-07-19 19:14:43      3.0         NaT   \n",
      "\n",
      "  OwnerDisplayName LastEditorDisplayName  \n",
      "0             None                  None  \n",
      "1             None                  None  \n",
      "2             None                  None  \n",
      "3             None                  None  \n",
      "4             None                  None  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_posts = pd.read_sql_query('SELECT * FROM posts', engine)\n",
    "print(raw_posts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Rename Id column to postId and OwnerUserId to userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   postId  PostTypeId  AcceptedAnswerId         CreaionDate  Score  ViewCount  \\\n",
      "0       1           1              15.0 2010-07-19 19:12:12     23     1278.0   \n",
      "1       2           1              59.0 2010-07-19 19:12:57     22     8198.0   \n",
      "2       3           1               5.0 2010-07-19 19:13:28     54     3613.0   \n",
      "3       4           1             135.0 2010-07-19 19:13:31     13     5224.0   \n",
      "4       5           2               NaN 2010-07-19 19:14:43     81        NaN   \n",
      "\n",
      "                                                Body  userId  \\\n",
      "0  <p>How should I elicit prior distributions fro...     8.0   \n",
      "1  <p>In many different statistical methods there...    24.0   \n",
      "2  <p>What are some valuable Statistical Analysis...    18.0   \n",
      "3  <p>I have two groups of data.  Each with a dif...    23.0   \n",
      "4  <p>The R-project</p>\\n\\n<p><a href=\"http://www...    23.0   \n",
      "\n",
      "      LasActivityDate                                              Title  ...  \\\n",
      "0 2010-09-15 21:08:26                      Eliciting priors from experts  ...   \n",
      "1 2012-11-12 09:21:54                                 What is normality?  ...   \n",
      "2 2013-05-27 14:48:36  What are some valuable Statistical Analysis op...  ...   \n",
      "3 2010-09-08 03:00:19  Assessing the significance of differences in d...  ...   \n",
      "4 2010-07-19 19:21:15                                               None  ...   \n",
      "\n",
      "  AnswerCount  CommentCount  FavoriteCount  LastEditorUserId  \\\n",
      "0         5.0             1           14.0               NaN   \n",
      "1         7.0             1            8.0              88.0   \n",
      "2        19.0             4           36.0             183.0   \n",
      "3         5.0             2            2.0               NaN   \n",
      "4         NaN             3            NaN              23.0   \n",
      "\n",
      "         LastEditDate  CommunityOwnedDate ParentId  ClosedDate  \\\n",
      "0                 NaT                 NaT      NaN         NaT   \n",
      "1 2010-08-07 17:56:44                 NaT      NaN         NaT   \n",
      "2 2011-02-12 05:50:03 2010-07-19 19:13:28      NaN         NaT   \n",
      "3                 NaT                 NaT      NaN         NaT   \n",
      "4 2010-07-19 19:21:15 2010-07-19 19:14:43      3.0         NaT   \n",
      "\n",
      "  OwnerDisplayName LastEditorDisplayName  \n",
      "0             None                  None  \n",
      "1             None                  None  \n",
      "2             None                  None  \n",
      "3             None                  None  \n",
      "4             None                  None  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "clean_posts = raw_posts.rename(columns = \n",
    "    {\n",
    "        'Id':'postId',\n",
    "        'OwnerUserId':'userId'\n",
    "    }\n",
    ")\n",
    "# print(raw_posts.head())\n",
    "# print(\"\\nAnd my clean ones\")\n",
    "print(clean_posts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Define new dataframes for users and posts with the following selected columns:\n",
    "    **users columns**: userId, Reputation,Views,UpVotes,DownVotes\n",
    "    **posts columns**: postId, Score,userID,ViewCount,CommentCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  Reputation  Views  UpVotes  DownVotes\n",
      "0      -1           1      0     5007       1920\n",
      "1       2         101     25        3          0\n",
      "2       3         101     22       19          0\n",
      "3       4         101     11        0          0\n",
      "4       5        6792   1145      662          5\n",
      "   postId  Score  userId  ViewCount  CommentCount\n",
      "0       1     23     8.0     1278.0             1\n",
      "1       2     22    24.0     8198.0             1\n",
      "2       3     54    18.0     3613.0             4\n",
      "3       4     13    23.0     5224.0             2\n",
      "4       5     81    23.0        NaN             3\n"
     ]
    }
   ],
   "source": [
    "# create a sub-dataframe from the existing, not a new SQL query\n",
    "new_users = users_with_id[['userId', 'Reputation', 'Views', 'UpVotes', 'DownVotes']]\n",
    "print(new_users.head())\n",
    "\n",
    "new_posts = clean_posts[['postId', 'Score', 'userId', 'ViewCount', 'CommentCount']]\n",
    "print(new_posts.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Merge both dataframes, users and posts. \n",
    "You will need to make a [merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) of posts and users dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  Reputation  Views  UpVotes  DownVotes  postId  Score  ViewCount  \\\n",
      "0      -1           1      0     5007       1920    2175      0        NaN   \n",
      "1      -1           1      0     5007       1920    8576      0        NaN   \n",
      "2      -1           1      0     5007       1920    8578      0        NaN   \n",
      "3      -1           1      0     5007       1920    8981      0        NaN   \n",
      "4      -1           1      0     5007       1920    8982      0        NaN   \n",
      "\n",
      "   CommentCount  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n"
     ]
    }
   ],
   "source": [
    "# merge them on userId\n",
    "users_and_posts = new_users.merge(new_posts, on='userId')\n",
    "print(users_and_posts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. How many missing values do you have in your merged dataframe? On which columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId              0\n",
       "Reputation          0\n",
       "Views               0\n",
       "UpVotes             0\n",
       "DownVotes           0\n",
       "postId              0\n",
       "Score               0\n",
       "ViewCount       48396\n",
       "CommentCount        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_and_posts.isnull().sum() #my solution so far\n",
    "# null_cols = users_and_posts.isnull().sum()\n",
    "# null_cols[null_cols > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. You will need to make something with missing values.  Will you clean or filling them? Explain. \n",
    "**Remember** to check the results of your code before passing to the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId          0\n",
      "Reputation      0\n",
      "Views           0\n",
      "UpVotes         0\n",
      "DownVotes       0\n",
      "postId          0\n",
      "Score           0\n",
      "ViewCount       0\n",
      "CommentCount    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# drop the rows where ViewCount is null. We have a ton of rows of data otherwise\n",
    "# uandp_nonulls = users_and_posts.drop(labels = users_and_posts.loc[users_and_posts['ViewCount'].isnull()])\n",
    "# print(uandp_nonulls.head())\n",
    "# print(list(users_and_posts))\n",
    "null_rows = users_and_posts[ users_and_posts['ViewCount'].isnull()].index\n",
    "uandp_nonulls = users_and_posts.drop(null_rows)\n",
    "# print(uandp_nonulls.describe)\n",
    "print(uandp_nonulls.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Adjust the data types in order to avoid future issues. Which ones should be changed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId            int64\n",
      "Reputation        int64\n",
      "Views             int64\n",
      "UpVotes           int64\n",
      "DownVotes         int64\n",
      "postId            int64\n",
      "Score             int64\n",
      "ViewCount       float64\n",
      "CommentCount      int64\n",
      "dtype: object\n",
      "\n",
      " And now I have adjusted that column so the entire table is Integer Datatype\n",
      "userId          int64\n",
      "Reputation      int64\n",
      "Views           int64\n",
      "UpVotes         int64\n",
      "DownVotes       int64\n",
      "postId          int64\n",
      "Score           int64\n",
      "ViewCount       int64\n",
      "CommentCount    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(uandp_nonulls.dtypes)\n",
    "# ViewCount is now a float, while the rest of the columns are int.\n",
    "\n",
    "uandp_no_nulls_all_int = uandp_nonulls[:].astype('int', inplace = True)\n",
    "print(\"\\n And now I have adjusted that column so the entire table is Integer Datatype\")\n",
    "print(uandp_no_nulls_all_int.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: Identify extreme values in your merged dataframe as you have learned in class, create a dataframe called outliers with the same columns as our data set and calculate the bounds. The values of the outliers dataframe will be the values of the merged_df that fall outside that bounds. You will need to save your outliers dataframe to a csv file on your-code folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uandp_no_nulls_all_int.describe().transpose()\n",
    "\n",
    "summary = uandp_no_nulls_all_int.describe().transpose()\n",
    "# print(summary)\n",
    "summary['IQR'] = summary['75%'] - summary['25%']\n",
    "# print(summary)\n",
    "\n",
    "outliers_table = pd.DataFrame(columns = uandp_no_nulls_all_int.columns)\n",
    "\n",
    "for col in summary.index:\n",
    "    iqr = summary.at[col, 'IQR']\n",
    "    cutoff = iqr * 1.5\n",
    "    lowerbound = summary.at[col, '25%']\n",
    "    upperbound = summary.at[col, '75%']\n",
    "    results = uandp_no_nulls_all_int[\n",
    "        (uandp_no_nulls_all_int[col] < lowerbound) |\n",
    "        (uandp_no_nulls_all_int[col] > upperbound)\n",
    "    ].copy()\n",
    "    results['Outlier'] = col\n",
    "    outliers_table = outliers_table.append(results)\n",
    "\n",
    "outliers_table.to_csv('outliers.csv', sep=',', index=False)\n",
    "# this results in 'userId' being an \"outlier\". how can I tell the code to skip this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
