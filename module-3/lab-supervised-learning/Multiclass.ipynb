{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37064bit212878bbfae34438ab1b845150e1f853",
   "display_name": "Python 3.7.0 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Once we've loaded our baseline dependencies, we'll load the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-47</th>\n      <th>0</th>\n      <th>-5</th>\n      <th>-27</th>\n      <th>-11</th>\n      <th>-19</th>\n      <th>-2</th>\n      <th>-33</th>\n      <th>37</th>\n      <th>0.1</th>\n      <th>...</th>\n      <th>0.3</th>\n      <th>2.1</th>\n      <th>-4.3</th>\n      <th>-3.2</th>\n      <th>4</th>\n      <th>-23.1</th>\n      <th>-68</th>\n      <th>15</th>\n      <th>C</th>\n      <th>0.4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-2</td>\n      <td>-3</td>\n      <td>-1</td>\n      <td>-3</td>\n      <td>39</td>\n      <td>10</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>...</td>\n      <td>-7</td>\n      <td>-2</td>\n      <td>1</td>\n      <td>6</td>\n      <td>-2</td>\n      <td>4</td>\n      <td>22</td>\n      <td>2</td>\n      <td>C</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>-4</td>\n      <td>-2</td>\n      <td>-6</td>\n      <td>-20</td>\n      <td>20</td>\n      <td>-1</td>\n      <td>-3</td>\n      <td>20</td>\n      <td>3</td>\n      <td>...</td>\n      <td>14</td>\n      <td>2</td>\n      <td>-2</td>\n      <td>0</td>\n      <td>23</td>\n      <td>21</td>\n      <td>-3</td>\n      <td>22</td>\n      <td>A</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>2</td>\n      <td>1</td>\n      <td>-7</td>\n      <td>-8</td>\n      <td>12</td>\n      <td>41</td>\n      <td>21</td>\n      <td>-14</td>\n      <td>3</td>\n      <td>...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>-2</td>\n      <td>-8</td>\n      <td>-29</td>\n      <td>29</td>\n      <td>7</td>\n      <td>C</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>-3</td>\n      <td>-2</td>\n      <td>-6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>10</td>\n      <td>-6</td>\n      <td>4</td>\n      <td>0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>0</td>\n      <td>-3</td>\n      <td>-8</td>\n      <td>-11</td>\n      <td>-2</td>\n      <td>19</td>\n      <td>4</td>\n      <td>C</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>11</td>\n      <td>11</td>\n      <td>9</td>\n      <td>-21</td>\n      <td>-17</td>\n      <td>-20</td>\n      <td>-7</td>\n      <td>...</td>\n      <td>-5</td>\n      <td>-4</td>\n      <td>-5</td>\n      <td>-14</td>\n      <td>-25</td>\n      <td>-3</td>\n      <td>-34</td>\n      <td>-2</td>\n      <td>C</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 66 columns</p>\n</div>",
      "text/plain": "   -47  0  -5  -27  -11  -19  -2  -33  37  0.1  ...  0.3  2.1  -4.3  -3.2   4  \\\n0    1  1  -2   -3   -1   -3  39   10   0   -1  ...   -7   -2     1     6  -2   \n1    1 -4  -2   -6  -20   20  -1   -3  20    3  ...   14    2    -2     0  23   \n2   10  2   1   -7   -8   12  41   21 -14    3  ...    2    1     2    -2  -8   \n3    1 -3  -2   -6    1    0  10   -6   4    0  ...    5    0    -3    -8 -11   \n4    3  0   4   11   11    9 -21  -17 -20   -7  ...   -5   -4    -5   -14 -25   \n\n   -23.1  -68  15  C  0.4  \n0      4   22   2  C    0  \n1     21   -3  22  A    1  \n2    -29   29   7  C    0  \n3     -2   19   4  C    0  \n4     -3  -34  -2  C    0  \n\n[5 rows x 66 columns]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8677 entries, 0 to 8676\nData columns (total 66 columns):\n-47      8677 non-null int64\n0        8677 non-null int64\n-5       8677 non-null int64\n-27      8677 non-null int64\n-11      8677 non-null int64\n-19      8677 non-null int64\n-2       8677 non-null int64\n-33      8677 non-null int64\n37       8677 non-null int64\n0.1      8677 non-null int64\n-4       8677 non-null int64\n-4.1     8677 non-null int64\n5        8677 non-null int64\n12       8677 non-null int64\n-28      8677 non-null int64\n16       8677 non-null int64\n-19.1    8677 non-null int64\n-5.1     8677 non-null int64\n-3       8677 non-null int64\n-10      8677 non-null int64\n-12      8677 non-null int64\n5.1      8677 non-null int64\n41       8677 non-null int64\n0.2      8677 non-null int64\n12.1     8677 non-null int64\n-9       8677 non-null int64\n-2.1     8677 non-null int64\n16.1     8677 non-null int64\n-11.1    8677 non-null int64\n-36      8677 non-null int64\n14       8677 non-null int64\n-10.1    8677 non-null int64\n9        8677 non-null int64\n7        8677 non-null int64\n2        8677 non-null int64\n-4.2     8677 non-null int64\n29       8677 non-null int64\n60       8677 non-null int64\n19       8677 non-null int64\n11       8677 non-null int64\n-14      8677 non-null int64\n-7       8677 non-null int64\n-2.2     8677 non-null int64\n5.2      8677 non-null int64\n-14.1    8677 non-null int64\n-27.1    8677 non-null int64\n-59      8677 non-null int64\n-13      8677 non-null int64\n-6       8677 non-null int64\n1        8677 non-null int64\n-3.1     8677 non-null int64\n-1       8677 non-null int64\n-10.2    8677 non-null int64\n3        8677 non-null int64\n20       8677 non-null int64\n-23      8677 non-null int64\n0.3      8677 non-null int64\n2.1      8677 non-null int64\n-4.3     8677 non-null int64\n-3.2     8677 non-null int64\n4        8677 non-null int64\n-23.1    8677 non-null int64\n-68      8677 non-null int64\n15       8677 non-null int64\nC        8677 non-null object\n0.4      8677 non-null int64\ndtypes: int64(65), object(1)\nmemory usage: 4.4+ MB\n"
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "D    2202\nC    2176\nA    2152\nB    2147\nName: C, dtype: int64"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['C'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2    2202\n0    2176\n1    2152\n3    2147\nName: 0.4, dtype: int64"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['0.4'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demostrating C column is the same as 0.4 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['numerical_C'] = df['C'].apply(lambda x:1 if 'A' in x else (2 if 'D' in x else (3 if 'B' in x else 0)))\n",
    "df['numerical_C'].equals(df['0.4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = 'C', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-47</th>\n      <th>0</th>\n      <th>-5</th>\n      <th>-27</th>\n      <th>-11</th>\n      <th>-19</th>\n      <th>-2</th>\n      <th>-33</th>\n      <th>37</th>\n      <th>0.1</th>\n      <th>...</th>\n      <th>0.3</th>\n      <th>2.1</th>\n      <th>-4.3</th>\n      <th>-3.2</th>\n      <th>4</th>\n      <th>-23.1</th>\n      <th>-68</th>\n      <th>15</th>\n      <th>0.4</th>\n      <th>numerical_C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-47</th>\n      <td>1.000000</td>\n      <td>0.244662</td>\n      <td>0.046450</td>\n      <td>-0.000303</td>\n      <td>-0.020278</td>\n      <td>0.014931</td>\n      <td>0.130753</td>\n      <td>0.237063</td>\n      <td>-0.358387</td>\n      <td>-0.071917</td>\n      <td>...</td>\n      <td>0.005695</td>\n      <td>0.004693</td>\n      <td>0.000854</td>\n      <td>-0.006709</td>\n      <td>-0.014897</td>\n      <td>-0.012629</td>\n      <td>-0.025135</td>\n      <td>-0.011030</td>\n      <td>0.003047</td>\n      <td>0.003047</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.244662</td>\n      <td>1.000000</td>\n      <td>0.333370</td>\n      <td>0.081316</td>\n      <td>-0.062082</td>\n      <td>-0.021186</td>\n      <td>0.076302</td>\n      <td>0.080824</td>\n      <td>-0.068678</td>\n      <td>-0.273657</td>\n      <td>...</td>\n      <td>-0.011692</td>\n      <td>0.016175</td>\n      <td>0.015352</td>\n      <td>-0.004672</td>\n      <td>-0.018734</td>\n      <td>0.000175</td>\n      <td>-0.012862</td>\n      <td>-0.008055</td>\n      <td>-0.003710</td>\n      <td>-0.003710</td>\n    </tr>\n    <tr>\n      <th>-5</th>\n      <td>0.046450</td>\n      <td>0.333370</td>\n      <td>1.000000</td>\n      <td>0.509942</td>\n      <td>-0.059332</td>\n      <td>0.017919</td>\n      <td>0.094762</td>\n      <td>0.111097</td>\n      <td>-0.028504</td>\n      <td>-0.128111</td>\n      <td>...</td>\n      <td>-0.004290</td>\n      <td>-0.029781</td>\n      <td>-0.019630</td>\n      <td>-0.011463</td>\n      <td>0.012742</td>\n      <td>0.005034</td>\n      <td>-0.006187</td>\n      <td>-0.015619</td>\n      <td>-0.007724</td>\n      <td>-0.007724</td>\n    </tr>\n    <tr>\n      <th>-27</th>\n      <td>-0.000303</td>\n      <td>0.081316</td>\n      <td>0.509942</td>\n      <td>1.000000</td>\n      <td>0.046765</td>\n      <td>0.051033</td>\n      <td>0.025919</td>\n      <td>0.041150</td>\n      <td>-0.009320</td>\n      <td>-0.051865</td>\n      <td>...</td>\n      <td>0.004772</td>\n      <td>-0.006138</td>\n      <td>-0.025217</td>\n      <td>-0.012400</td>\n      <td>0.017559</td>\n      <td>0.003528</td>\n      <td>0.015161</td>\n      <td>-0.000679</td>\n      <td>-0.004700</td>\n      <td>-0.004700</td>\n    </tr>\n    <tr>\n      <th>-11</th>\n      <td>-0.020278</td>\n      <td>-0.062082</td>\n      <td>-0.059332</td>\n      <td>0.046765</td>\n      <td>1.000000</td>\n      <td>0.502534</td>\n      <td>0.089337</td>\n      <td>0.026372</td>\n      <td>-0.000580</td>\n      <td>0.005375</td>\n      <td>...</td>\n      <td>-0.018070</td>\n      <td>0.010031</td>\n      <td>-0.004485</td>\n      <td>-0.002881</td>\n      <td>-0.012462</td>\n      <td>-0.015638</td>\n      <td>-0.007952</td>\n      <td>-0.019441</td>\n      <td>-0.007274</td>\n      <td>-0.007274</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>-23.1</th>\n      <td>-0.012629</td>\n      <td>0.000175</td>\n      <td>0.005034</td>\n      <td>0.003528</td>\n      <td>-0.015638</td>\n      <td>-0.023114</td>\n      <td>0.006044</td>\n      <td>0.000823</td>\n      <td>0.008977</td>\n      <td>0.008685</td>\n      <td>...</td>\n      <td>0.024791</td>\n      <td>-0.016501</td>\n      <td>0.006334</td>\n      <td>0.033311</td>\n      <td>0.501496</td>\n      <td>1.000000</td>\n      <td>0.241874</td>\n      <td>0.132446</td>\n      <td>-0.003925</td>\n      <td>-0.003925</td>\n    </tr>\n    <tr>\n      <th>-68</th>\n      <td>-0.025135</td>\n      <td>-0.012862</td>\n      <td>-0.006187</td>\n      <td>0.015161</td>\n      <td>-0.007952</td>\n      <td>-0.030958</td>\n      <td>-0.027018</td>\n      <td>-0.025000</td>\n      <td>0.021347</td>\n      <td>-0.011320</td>\n      <td>...</td>\n      <td>0.107885</td>\n      <td>0.058487</td>\n      <td>0.077899</td>\n      <td>0.039891</td>\n      <td>0.112691</td>\n      <td>0.241874</td>\n      <td>1.000000</td>\n      <td>0.454316</td>\n      <td>0.025736</td>\n      <td>0.025736</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-0.011030</td>\n      <td>-0.008055</td>\n      <td>-0.015619</td>\n      <td>-0.000679</td>\n      <td>-0.019441</td>\n      <td>-0.033007</td>\n      <td>-0.036115</td>\n      <td>-0.043304</td>\n      <td>0.007626</td>\n      <td>-0.009164</td>\n      <td>...</td>\n      <td>0.222153</td>\n      <td>0.100709</td>\n      <td>0.115989</td>\n      <td>0.078022</td>\n      <td>0.031608</td>\n      <td>0.132446</td>\n      <td>0.454316</td>\n      <td>1.000000</td>\n      <td>0.020699</td>\n      <td>0.020699</td>\n    </tr>\n    <tr>\n      <th>0.4</th>\n      <td>0.003047</td>\n      <td>-0.003710</td>\n      <td>-0.007724</td>\n      <td>-0.004700</td>\n      <td>-0.007274</td>\n      <td>-0.021501</td>\n      <td>0.030203</td>\n      <td>-0.006395</td>\n      <td>-0.011385</td>\n      <td>-0.009506</td>\n      <td>...</td>\n      <td>0.001201</td>\n      <td>-0.000582</td>\n      <td>0.023222</td>\n      <td>0.003176</td>\n      <td>-0.000117</td>\n      <td>-0.003925</td>\n      <td>0.025736</td>\n      <td>0.020699</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>numerical_C</th>\n      <td>0.003047</td>\n      <td>-0.003710</td>\n      <td>-0.007724</td>\n      <td>-0.004700</td>\n      <td>-0.007274</td>\n      <td>-0.021501</td>\n      <td>0.030203</td>\n      <td>-0.006395</td>\n      <td>-0.011385</td>\n      <td>-0.009506</td>\n      <td>...</td>\n      <td>0.001201</td>\n      <td>-0.000582</td>\n      <td>0.023222</td>\n      <td>0.003176</td>\n      <td>-0.000117</td>\n      <td>-0.003925</td>\n      <td>0.025736</td>\n      <td>0.020699</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>66 rows Ã— 66 columns</p>\n</div>",
      "text/plain": "                  -47         0        -5       -27       -11       -19  \\\n-47          1.000000  0.244662  0.046450 -0.000303 -0.020278  0.014931   \n0            0.244662  1.000000  0.333370  0.081316 -0.062082 -0.021186   \n-5           0.046450  0.333370  1.000000  0.509942 -0.059332  0.017919   \n-27         -0.000303  0.081316  0.509942  1.000000  0.046765  0.051033   \n-11         -0.020278 -0.062082 -0.059332  0.046765  1.000000  0.502534   \n...               ...       ...       ...       ...       ...       ...   \n-23.1       -0.012629  0.000175  0.005034  0.003528 -0.015638 -0.023114   \n-68         -0.025135 -0.012862 -0.006187  0.015161 -0.007952 -0.030958   \n15          -0.011030 -0.008055 -0.015619 -0.000679 -0.019441 -0.033007   \n0.4          0.003047 -0.003710 -0.007724 -0.004700 -0.007274 -0.021501   \nnumerical_C  0.003047 -0.003710 -0.007724 -0.004700 -0.007274 -0.021501   \n\n                   -2       -33        37       0.1  ...       0.3       2.1  \\\n-47          0.130753  0.237063 -0.358387 -0.071917  ...  0.005695  0.004693   \n0            0.076302  0.080824 -0.068678 -0.273657  ... -0.011692  0.016175   \n-5           0.094762  0.111097 -0.028504 -0.128111  ... -0.004290 -0.029781   \n-27          0.025919  0.041150 -0.009320 -0.051865  ...  0.004772 -0.006138   \n-11          0.089337  0.026372 -0.000580  0.005375  ... -0.018070  0.010031   \n...               ...       ...       ...       ...  ...       ...       ...   \n-23.1        0.006044  0.000823  0.008977  0.008685  ...  0.024791 -0.016501   \n-68         -0.027018 -0.025000  0.021347 -0.011320  ...  0.107885  0.058487   \n15          -0.036115 -0.043304  0.007626 -0.009164  ...  0.222153  0.100709   \n0.4          0.030203 -0.006395 -0.011385 -0.009506  ...  0.001201 -0.000582   \nnumerical_C  0.030203 -0.006395 -0.011385 -0.009506  ...  0.001201 -0.000582   \n\n                 -4.3      -3.2         4     -23.1       -68        15  \\\n-47          0.000854 -0.006709 -0.014897 -0.012629 -0.025135 -0.011030   \n0            0.015352 -0.004672 -0.018734  0.000175 -0.012862 -0.008055   \n-5          -0.019630 -0.011463  0.012742  0.005034 -0.006187 -0.015619   \n-27         -0.025217 -0.012400  0.017559  0.003528  0.015161 -0.000679   \n-11         -0.004485 -0.002881 -0.012462 -0.015638 -0.007952 -0.019441   \n...               ...       ...       ...       ...       ...       ...   \n-23.1        0.006334  0.033311  0.501496  1.000000  0.241874  0.132446   \n-68          0.077899  0.039891  0.112691  0.241874  1.000000  0.454316   \n15           0.115989  0.078022  0.031608  0.132446  0.454316  1.000000   \n0.4          0.023222  0.003176 -0.000117 -0.003925  0.025736  0.020699   \nnumerical_C  0.023222  0.003176 -0.000117 -0.003925  0.025736  0.020699   \n\n                  0.4  numerical_C  \n-47          0.003047     0.003047  \n0           -0.003710    -0.003710  \n-5          -0.007724    -0.007724  \n-27         -0.004700    -0.004700  \n-11         -0.007274    -0.007274  \n...               ...          ...  \n-23.1       -0.003925    -0.003925  \n-68          0.025736     0.025736  \n15           0.020699     0.020699  \n0.4          1.000000     1.000000  \nnumerical_C  1.000000     1.000000  \n\n[66 rows x 66 columns]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that numerical_C and 0.4 have a correlation of 1. Therefore, we should drop one of the two to avoid redundant information. Also we have to check if there are other features with hard correlation and drop them too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "tri = np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool)\n",
    "#Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(tri)\n",
    "# Find index of feature columns with correlation greater than 90%\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.9)]\n",
    "# Drop highly colinear features\n",
    "df.drop(df[to_drop], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(8677, 65)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "-47      0\n0        0\n-5       0\n-27      0\n-11      0\n        ..\n4        0\n-23.1    0\n-68      0\n15       0\n0.4      0\nLength: 65, dtype: int64"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>-47</th>\n      <th>0</th>\n      <th>-5</th>\n      <th>-27</th>\n      <th>-11</th>\n      <th>-19</th>\n      <th>-2</th>\n      <th>-33</th>\n      <th>37</th>\n      <th>0.1</th>\n      <th>...</th>\n      <th>20</th>\n      <th>-23</th>\n      <th>0.3</th>\n      <th>2.1</th>\n      <th>-4.3</th>\n      <th>-3.2</th>\n      <th>4</th>\n      <th>-23.1</th>\n      <th>-68</th>\n      <th>15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>-2</td>\n      <td>-3</td>\n      <td>-1</td>\n      <td>-3</td>\n      <td>39</td>\n      <td>10</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>...</td>\n      <td>54</td>\n      <td>17</td>\n      <td>-7</td>\n      <td>-2</td>\n      <td>1</td>\n      <td>6</td>\n      <td>-2</td>\n      <td>4</td>\n      <td>22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>-4</td>\n      <td>-2</td>\n      <td>-6</td>\n      <td>-20</td>\n      <td>20</td>\n      <td>-1</td>\n      <td>-3</td>\n      <td>20</td>\n      <td>3</td>\n      <td>...</td>\n      <td>-6</td>\n      <td>-7</td>\n      <td>14</td>\n      <td>2</td>\n      <td>-2</td>\n      <td>0</td>\n      <td>23</td>\n      <td>21</td>\n      <td>-3</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>2</td>\n      <td>1</td>\n      <td>-7</td>\n      <td>-8</td>\n      <td>12</td>\n      <td>41</td>\n      <td>21</td>\n      <td>-14</td>\n      <td>3</td>\n      <td>...</td>\n      <td>-22</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>-2</td>\n      <td>-8</td>\n      <td>-29</td>\n      <td>29</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>-3</td>\n      <td>-2</td>\n      <td>-6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>10</td>\n      <td>-6</td>\n      <td>4</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-9</td>\n      <td>-7</td>\n      <td>5</td>\n      <td>0</td>\n      <td>-3</td>\n      <td>-8</td>\n      <td>-11</td>\n      <td>-2</td>\n      <td>19</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>11</td>\n      <td>11</td>\n      <td>9</td>\n      <td>-21</td>\n      <td>-17</td>\n      <td>-20</td>\n      <td>-7</td>\n      <td>...</td>\n      <td>-64</td>\n      <td>-11</td>\n      <td>-5</td>\n      <td>-4</td>\n      <td>-5</td>\n      <td>-14</td>\n      <td>-25</td>\n      <td>-3</td>\n      <td>-34</td>\n      <td>-2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 64 columns</p>\n</div>",
      "text/plain": "   -47  0  -5  -27  -11  -19  -2  -33  37  0.1  ...  20  -23  0.3  2.1  -4.3  \\\n0    1  1  -2   -3   -1   -3  39   10   0   -1  ...  54   17   -7   -2     1   \n1    1 -4  -2   -6  -20   20  -1   -3  20    3  ...  -6   -7   14    2    -2   \n2   10  2   1   -7   -8   12  41   21 -14    3  ... -22    3    2    1     2   \n3    1 -3  -2   -6    1    0  10   -6   4    0  ...  -9   -7    5    0    -3   \n4    3  0   4   11   11    9 -21  -17 -20   -7  ... -64  -11   -5   -4    -5   \n\n   -3.2   4  -23.1  -68  15  \n0     6  -2      4   22   2  \n1     0  23     21   -3  22  \n2    -2  -8    -29   29   7  \n3    -8 -11     -2   19   4  \n4   -14 -25     -3  -34  -2  \n\n[5 rows x 64 columns]"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df[[col for col in df if col != '0.4']]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['0.4']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modeling and Evaluation with all features of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    features,\n",
    "    target,\n",
    "    test_size = .2,\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.4619815668202765"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bernoulli = BernoulliNB().fit(train_X, train_y)\n",
    "bernoulli.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.875"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gaussian = GaussianNB().fit(train_X, train_y)\n",
    "gaussian.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.3456221198156682"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_regr = LogisticRegression(max_iter = 1000)\n",
    "log_regr.fit(train_X, train_y.values.ravel())\n",
    "log_regr.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.6866359447004609"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_X, train_y.values.ravel())\n",
    "knn.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.7753456221198156"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scalar = RobustScaler().fit(train_X)\n",
    "knn_robust = KNeighborsClassifier()\n",
    "knn_robust.fit(scalar.transform(train_X), train_y.values.ravel())\n",
    "knn_robust.score(scalar.transform(test_X), test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9193548387096774"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ks_rf = RandomForestClassifier().fit(train_X, train_y)\n",
    "ks_rf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.\n",
    "\n",
    "Having too many irrelevant features in your data can decrease the accuracy of the models. Three benefits of performing feature selection before modeling your data are:\n",
    "\n",
    "* Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "* Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "* Reduces Training Time: Less data means that algorithms train faster.\n",
    "\n",
    "Two different feature selection methods provided by the scikit-learn Python library are Recursive Feature Elimination and feature importance ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RFE & RandomForestClassifier\n",
    "\n",
    "RandomForestClassifier: A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n",
    "\n",
    "Recursive Feature Elimination (RFE): Feature ranking with recursive feature elimination.\n",
    "\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9078341013824884"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "log_regr = RandomForestClassifier()\n",
    "\n",
    "# create the RFE model and select 30 attributes\n",
    "rfe = RFE(log_regr, 30)\n",
    "\n",
    "rfe.fit(train_X, train_y.values.ravel())\n",
    "rfe.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-47c708a57e04>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-47c708a57e04>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    Linear Support Vector Classification: Similar to SVC with parameter kernel=â€™linearâ€™, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### SelectFromModel & LogisticRegression\n",
    "\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the â€˜multi_classâ€™ option is set to â€˜ovrâ€™, and uses the cross-entropy loss if the â€˜multi_classâ€™ option is set to â€˜multinomialâ€™. (Currently the â€˜multinomialâ€™ option is supported only by the â€˜lbfgsâ€™, â€˜sagâ€™, â€˜sagaâ€™ and â€˜newton-cgâ€™ solvers.)\n",
    "\n",
    "SelectFromModel: Meta-transformer for selecting features based on importance weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(6941, 22)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = SelectFromModel(estimator=LogisticRegression(max_iter = 1000)).fit(train_X, train_y)\n",
    "new_test_X = model.transform(train_X)\n",
    "new_test_X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8277649769585254"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(estimator=LogisticRegression(max_iter = 1000))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "pipe = clf.fit(train_X, train_y)\n",
    "pipe.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SelectFromModel & LinearSVC\n",
    "\n",
    "Linear Support Vector Classification: Similar to SVC with parameter kernel=â€™linearâ€™, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n",
    "\n",
    "This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.\n",
    "\n",
    "SelectFromModel: Meta-transformer for selecting features based on importance weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(6941, 64)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = SelectFromModel(estimator=LinearSVC(C=0.01, penalty=\"l1\", dual=False)).fit(train_X, train_y)\n",
    "new_test_X = model.transform(train_X)\n",
    "new_test_X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9147465437788018"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(C=0.01, penalty=\"l1\", dual=False))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "pipe = clf.fit(train_X, train_y)\n",
    "pipe.score(test_X, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}